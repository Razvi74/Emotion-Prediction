{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "952c0ade-c315-4c24-976c-2e1a7003ed4a",
   "metadata": {},
   "source": [
    "# Emotion Prediction from a Given Picture Using FER2013 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670d72a3-d542-4c9d-8ed6-fcfe9876a512",
   "metadata": {},
   "source": [
    "# Importing Libraries & Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec4e960-5853-4c33-93d7-aa625e7cd165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping,ModelCheckpoint,LearningRateScheduler\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, optimizers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import backend as k\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ba4607-66bb-4279-977f-e1f591e125aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('icml_face_data.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d801f2-1287-4d8a-84ae-852df119886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['emotion','Usage','pixels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286f859c-d2a1-48b8-a25e-1efd1d7a773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['pixels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95445168-83a5-431c-b625-1929ceda9f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string2array(x):\n",
    "    return np.array(x.split(' ')).reshape(48,48).astype('float32')\n",
    "df['pixels']=df['pixels'].apply(lambda x: string2array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3dd8ae-c69a-42b8-9205-8d52d5500649",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cebe3f4-86e8-438b-99c0-c971e1d505fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataframe's rows\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78efbe8f-3f78-452c-9215-5c662dc36579",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.emotion.value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d01f7cb-bdb7-4a82-a658-e7e5c34ad2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_text={0: 'anger', 1: 'disgust', 2:'fear', 3:'happiness', 4: 'sad', 5: 'surprised', 6:'neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7aebe8-cbfd-4e94-85dd-0da634069723",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(hue=[label_to_text[i] for i in df.emotion.value_counts().index], x=[label_to_text[i] for i in df.emotion.value_counts().index],\n",
    "           y=df.emotion.value_counts(), palette='pastel',legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9bead8-969c-410b-9ce0-c83cf5ac62dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.emotion.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3becb63-d670-477f-81e8-46eed0f4e6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6841ebb1-69db-4ee9-b8ce-ba579d6722a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing pictures of training set with corresponding labels\n",
    "label_to_text={0: 'anger', 1: 'disgust', 2:'fear', 3:'happiness', 4: 'sad', 5: 'surprised', 6:'neutral'}\n",
    "count=0\n",
    "fig,axs=plt.subplots(7,7,figsize=(16,16))\n",
    "for i in df.emotion.unique():\n",
    "    data=df[df['emotion']==i]\n",
    "    for img in data['pixels']:\n",
    "        axs[i][count].imshow(img,cmap='gray')\n",
    "        axs[i][count].title.set_fontsize(14)\n",
    "        axs[i][count].title.set_fontweight('bold')\n",
    "        axs[i][count].title.set_color('red')\n",
    "        axs[i][count].title.set_backgroundcolor('yellow')\n",
    "        axs[i][count].title.set_text(label_to_text[i])\n",
    "        count+=1\n",
    "        if count==7:\n",
    "           break;\n",
    "    count=0\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad117fd8-231a-4846-bd4f-f6a2083f05ab",
   "metadata": {},
   "source": [
    "# Data Preparation & Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633f7392-e6a5-4725-b1f5-a55d4d1ebf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataframe into features and labels\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480fc09d-650d-41fe-8950-14a8c1949b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['pixels']\n",
    "y=to_categorical(df['emotion'])\n",
    "X.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0b7e90-520b-40e0-94d1-5afa9393daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36a21e5-0563-4577-bcf7-97fff354ac9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c9f2fa-3cba-41e7-ac6c-b7aeccb1bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack(X,axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c243221d-e67b-4bdb-a1c0-2ef2451ef43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack(X,axis=0).shape\n",
    "X=np.stack(X,axis=0)\n",
    "X=X.reshape(df.shape[0],48,48,1)\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304e8251-f8be-47b2-9d54-6e009128172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cc6c83-30c6-4e87-bd9f-490276455ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.15,random_state=42)\n",
    "X_test,X_val,y_test,y_val=train_test_split(X_val,y_val,test_size=0.15,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c712ee7-587a-4005-a98b-c9f5516a6753",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Set:', X_train.shape,y_train.shape)\n",
    "print('Val Set:', X_val.shape,y_val.shape)\n",
    "print('Test Set:', X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52cbb33-3930-4d05-92b1-9a39c663053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen=ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "train_set=train_datagen.flow(X_train,y_train,batch_size=64)\n",
    "val_test_datagen=ImageDataGenerator(rescale=1./255)\n",
    "val_set=val_test_datagen.flow(X_val,y_val,batch_size=64)\n",
    "test_set=val_test_datagen.flow(X_test,y_test,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc2b926-60c6-4a59-8039-d083e58fe61f",
   "metadata": {},
   "source": [
    "# Build and Train Deep Learning Model for Facial Expression Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754cfe5f-61f4-4c3c-a859-a3dc28c029d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_block(X,filter):\n",
    "    #convolutional block\n",
    "    X_copy=X\n",
    "    f1,f2,f3=filter\n",
    "    #Main path\n",
    "    X=Conv2D(f1,(1,1),strides=(1,1))(X)\n",
    "    X=MaxPool2D((2,2))(X)\n",
    "    X=BatchNormalization(axis=3)(X)\n",
    "    X=Activation('relu')(X)\n",
    "    X=Conv2D(f2,kernel_size=(3,3),strides=(1,1),padding='same')(X)\n",
    "    X=BatchNormalization(axis=3)(X)\n",
    "    X=Activation('relu')(X)\n",
    "    X=Conv2D(f3,kernel_size=(1,1),strides=(1,1),padding='same')(X)\n",
    "    X=BatchNormalization(axis=3)(X)\n",
    "\n",
    "    #Short path\n",
    "    X_copy=Conv2D(f3,kernel_size=(1,1),strides=(1,1))(X_copy)\n",
    "    X_copy=MaxPool2D((2,2))( X_copy)\n",
    "    X_copy=BatchNormalization(axis=3)(X_copy)\n",
    "    #Add\n",
    "    X=Add()([X,X_copy])\n",
    "    X=Activation('relu')(X)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53580bd6-96c1-4a18-80e9-0f0930a3e8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X,filter):\n",
    "    X_copy=X\n",
    "    f1,f2,f3=filter\n",
    "    #Main path\n",
    "    X=Conv2D(f1,(1,1),strides=(1,1))(X)\n",
    "    #X=MaxPool2D((2,2))(X)\n",
    "    X=BatchNormalization(axis=3)(X)\n",
    "    X=Activation('relu')(X)\n",
    "    X=Conv2D(f2,kernel_size=(3,3),strides=(1,1),padding='same')(X)\n",
    "    X=BatchNormalization(axis=3)(X)\n",
    "    X=Activation('relu')(X)\n",
    "    X=Conv2D(f3,kernel_size=(1,1),strides=(1,1),padding='same')(X)\n",
    "    X=BatchNormalization(axis=3)(X)\n",
    "    #Add\n",
    "    #X=Add()([X,X_copy])\n",
    "    X=Activation('relu')(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52f6e68-33ad-4635-8d1e-f0dc5bc753f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    input_shape=(48,48,1)\n",
    "    #input tensor shape\n",
    "    X_input=Input(input_shape)\n",
    "    #1 Stage\n",
    "    X=Conv2D(64,(3,3),padding='same')(X_input)\n",
    "    X=BatchNormalization(axis=3)(X)\n",
    "    X=Activation('relu')(X)\n",
    "\n",
    "    #2 Stage\n",
    "    X=res_block(X,filter=[64,128,32])\n",
    "    X=identity_block(X,filter=[64,128,32])\n",
    "\n",
    "    #3 stage\n",
    "    X=res_block(X,filter=[128,256,64])\n",
    "    X=identity_block(X,filter=[128,256,64])\n",
    "\n",
    "    #4 stage\n",
    "    X=res_block(X,filter=[256,512,128])\n",
    "    X=identity_block(X,filter=[256,512,128])\n",
    "\n",
    "    #Average Pooling\n",
    "    X=GlobalAveragePooling2D()(X)\n",
    "\n",
    "    #Final layer\n",
    "    X=Flatten()(X)\n",
    "    X=Dense(512,activation='relu')(X)\n",
    "    X=Dense(7,activation='softmax')(X)\n",
    "    model=Model(inputs=X_input,outputs=X)\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc8684-fc54-49da-9a63-2a384b4fac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(48,48,1)\n",
    "X_input=Input(input_shape)\n",
    "X_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee93a3be-4018-40a0-bb35-b12ec48e9e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    " X=Conv2D(64,(3,3),padding='same')(X_input)\n",
    " X=BatchNormalization(axis=3)(X)\n",
    " X=Activation('relu')(X)\n",
    " X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e1af6d-d127-4a79-891b-cd7ddf3159d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=res_block(X,filter=[64,128,32])\n",
    "X=identity_block(X,filter=[64,128,32])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e38d3e-3a75-4381-84ce-46ad3602818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "facial_model=get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25d835b-9b19-4ac3-9f93-fafb9e063195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pydot\n",
    "#!pip install graphviz --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354bd2b7-7d5e-44fd-9c98-4816bbb14332",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(facial_model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bbe2da-8bf9-410b-8432-b6ebc2f2f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pydot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d016652d-68ac-4b23-ad87-12a579a5de4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the network\n",
    "#facial_model.compile(optimizer='adamax',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#define callback functions\n",
    "#earlystopping=EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=30)\n",
    "#path='Facial_expression_weights.keras'\n",
    "#checkpointer=ModelCheckpoint(filepath=path,verbose=1,save_best_only=True)\n",
    "#reduce_lr=ReduceLROnPlateau(monitor='val_loss',factor=0.2,patience=15,verbose=1,mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b140f2ee-7f84-4f34-a323-231e14b62008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the network\n",
    "facial_model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#define callback functions\n",
    "earlystopping=EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=20)\n",
    "path='Facial_expression_weights.keras'\n",
    "checkpointer=ModelCheckpoint(filepath=path,verbose=1,save_best_only=True)\n",
    "reduce_lr=ReduceLROnPlateau(monitor='val_loss',factor=0.001,patience=3,verbose=1,mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d9c79-b506-40d0-96cc-2ababaceaec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment below for retraining.\n",
    "#h=facial_model.fit(train_set,validation_data=val_set,epochs=60,callbacks=[checkpointer,earlystopping,reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b1da4a-107d-480b-ae5d-537cc5622030",
   "metadata": {},
   "outputs": [],
   "source": [
    "facial_model.load_weights(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392940a9-f56c-4453-9c16-bc5dc3e9d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,acc=facial_model.evaluate(test_set)\n",
    "print(\"Accuracy on test set {:.2f}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6833f2-edff-421a-b16e-dd1dcc87b913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment below for retraining.\n",
    "#plt.figure(figsize=(12,6))\n",
    "#plt.subplot(1,2,1)\n",
    "#plt.plot(h.history['loss'])\n",
    "#plt.plot(h.history['val_loss'])\n",
    "#plt.title('Loss vs. Epoch')\n",
    "#plt.ylabel('Loss')\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.legend(['train','eval'])\n",
    "#plt.subplot(1,2,2)\n",
    "#plt.plot(h.history['accuracy'])\n",
    "#plt.plot(h.history['val_accuracy'])\n",
    "#plt.title('Accuracy vs Epoch')\n",
    "#plt.ylabel('Acc')\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.legend(['train','eval'])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb46bbac-6450-43e5-916e-2f7f3b0e1198",
   "metadata": {},
   "source": [
    "# Getting Prediction from a Given Picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55d2614-7fde-44bc-ae08-1c46d9ab93fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the picture. For better prediction, the face must be cropped.\n",
    "import tkinter\n",
    "from tkinter import filedialog\n",
    "file=tkinter.filedialog.askopenfilenames()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52387bba-c442-4e09-8189-1adce69cc434",
   "metadata": {},
   "outputs": [],
   "source": [
    "image2=cv2.imread(file[0])\n",
    "copy=image2.copy()\n",
    "copy=cv2.cvtColor(copy,cv2.COLOR_BGR2RGB)\n",
    "image2=cv2.cvtColor(image2,cv2.COLOR_BGR2GRAY)\n",
    "image2=cv2.resize(image2,(48,48))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d4bab5-a9c8-4026-9a45-7912ea5fea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "image2=image2/255.\n",
    "img_array = keras.utils.img_to_array(image2)\n",
    "img_array = keras.ops.expand_dims(img_array,0) \n",
    "pred=facial_model.predict(img_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d36fa-20b7-4129-bc61-5f493ddaa917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "max=0\n",
    "for i in range(0,6):\n",
    "    if operator.gt(pred[0,i],max):\n",
    "        max=pred[0,i]\n",
    "        idx=i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2674d7b8-d1ca-40a1-8150-bf653a99ea19",
   "metadata": {},
   "source": [
    "# Showing the Loaded Picture and the Predicted Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6c3a85-54e3-42df-be05-ca5a25c3d468",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.set_title('Prediciton={}\\n'.format(label_to_text[idx]))\n",
    "ax.title.set_backgroundcolor('yellow')\n",
    "ax.title.set_color('blue')\n",
    "ax.title.set_fontweight('bold')\n",
    "plt.imshow(copy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8f8f46-5dbd-4e90-9f42-278cfd6adb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377f1aca-b2c3-4f84-938d-89a8fc212e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fed318-72f1-472a-82ef-6f75c0d8ffa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96cee23-2e2e-475d-9366-6f0e6e7c798b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3797bd7c-0e88-4c98-837f-de00388d5a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e45024f-d88d-4245-87d6-af4e17cff1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286b1e3f-1eb2-4f8d-8f81-4605ab83dbe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba07934d-f6bc-4fb2-954f-ab2386f396a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae934e03-07de-4587-b858-573078380a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7eaf37-b9da-4757-803d-3cd56fdde296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d16df9-570c-4790-972e-60eeed64dddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54dd4e4-d577-4454-8113-a1a1e706b422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4644a186-46e2-4b8c-814b-6fe8fd98b17c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f64bd5e-176d-4443-9fc9-ba0d6711bded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f819b433-fe96-4b9a-8399-ba87edd71196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b5ec9e-00a5-47cf-a947-60a7f116080c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba453b8a-2055-489a-9509-bd2071a2406a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
